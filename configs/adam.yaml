# Adam - Modelo autónomo por coagente
# Configuración para RunPod Serverless
#
# Uso:
#   elchat train --config configs/adam.yaml
#   elchat train --config configs/adam.yaml --dry-run

version: "1.0"
name: "adam"

# RunPod Serverless Configuration
serverless:
  endpoint_id: null  # Set after creating endpoint in RunPod
  image: "ghcr.io/coagente/adam-worker:latest"

# Cloud Configuration (for Serverless)
cloud:
  provider: runpod_serverless
  gpu: "NVIDIA A100 80GB PCIe"  # 80GB for LFM2-2.6B with gradient checkpointing
  max_workers: 1

# Training Configuration
training:
  language: "es"
  base_model: "LiquidAI/LFM2-2.6B-Exp"
  target_tokens: 100_000_000  # 100M tokens
  
  # Memory optimizations
  device_batch_size: 1  # Keep low, use gradient accumulation
  gradient_accumulation_steps: 32  # Effective batch = 32 * 2048 = 65K tokens
  gradient_checkpointing: true  # -40% VRAM
  
  # Learning rate (low for fine-tuning)
  learning_rate: 2e-5
  weight_decay: 0.01
  
  # Evaluation
  eval_every: 100
  
  # Autonomy parameters
  stochastic_depth: 0.1
  noise_scale: 0.01
  mood_dim: 32
  stochastic_temp_var: 0.2

# Checkpointing (robustness)
checkpoints:
  enabled: true
  every: 100  # Save every 100 steps
  destination: "huggingface"  # "local" or "huggingface"
  hf_repo: "coagente/adam"  # HuggingFace repo for checkpoints
  resume_from: "auto"  # "auto" finds latest, or specific path

# Data
data:
  source: "fineweb2"
  num_shards: 5

# Cost Estimate
estimate:
  hours: 2.5
  cost_usd: 4.00  # A100 80GB Serverless ~$1.50/hr
